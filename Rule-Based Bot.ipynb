{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "411b258e-b894-4375-bea1-d961020d6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c28a356-5c55-483d-bccd-40db9695a52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is your name?\n",
      " Densey\n",
      "Hi Densey, I am Rule-Bot. Will you help me learn about your planet?\n",
      " yes\n",
      "Why are you here? simply\n",
      "How do you think I feel when you say that?\n",
      " no clue\n",
      "I see. Can you elaborate\n",
      " how do you feel\n",
      "Tell me more\n",
      " im excited\n",
      "Tell me more\n",
      " i am coming from planet udods\n",
      "Interesting, can you let me more\n",
      " describe planet intent\n",
      "I see. Can you elaborate\n",
      " describe_planet_intent\n",
      "I see. Can you elaborate\n",
      " quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok, have a nice Earth day!\n"
     ]
    }
   ],
   "source": [
    "#RULE BASED CHATBOT\n",
    "class RuleBot:\n",
    "    #potential negative responses\n",
    "    negative_responses = (\"no\",\"nah\",\"nope\",\"naw\",\"not a chance\",\"sorry\")\n",
    "    #exit conversation keywords\n",
    "    exit_commands = (\"quit\",\"pause\",\"exit\",\"goodbye\",\"bye\",\"later\")\n",
    "    #Random Starter Questions\n",
    "    random_questions = (\n",
    "    \"Why are you here?\",\n",
    "    \"Are there many humans like you?\",\n",
    "    \"What do you consume for sustenance?\",\n",
    "    \"Is there intelligent life on this planet?\",\n",
    "    \"Does Earth have a leader?\",\n",
    "    \"What planets have you visited?\",\n",
    "    \"What technology do you have on this planet?\")\n",
    "\n",
    "    def __init__(self):\n",
    "        self.alienbabble = {'describe_planet_intent':r'.*\\s*your planet.*',\n",
    "                            'answer_why_intent':r'why\\sare.*',\n",
    "                            'about_intellipat':r'.*\\s*intellipat',\n",
    "                            'about_session':r'.*\\s*session'}\n",
    "\n",
    "    def greet(self):\n",
    "        self.name = input(\"What is your name?\\n\")\n",
    "        will_help = input(\n",
    "            f\"Hi {self.name}, I am Rule-Bot. Will you help me learn about your planet?\\n\")\n",
    "        if will_help in self.negative_responses:\n",
    "            print(\"Ok, have a nice Earth day!\")\n",
    "            return\n",
    "        self.chat()\n",
    "    \n",
    "    def make_exit(self, reply):\n",
    "        for command in self.exit_commands:\n",
    "            if reply == command:\n",
    "                print(\"Ok, have a nice Earth day!\")\n",
    "                return True\n",
    "    \n",
    "    def chat(self):\n",
    "        reply = input(random.choice(self.random_questions)).lower()\n",
    "        while not self.make_exit(reply):\n",
    "            reply = input(self.match_reply(reply))\n",
    "    \n",
    "    def match_reply(self, reply):\n",
    "        for key,value in self.alienbabble.items():\n",
    "            intent = key\n",
    "            regex_pattern = value\n",
    "            found_match = re.match(regex_pattern, reply)\n",
    "            if found_match and intent == 'describe_planet_intent':\n",
    "                return self.describe_planet_intent()\n",
    "            elif found_match and intent == 'answer_why_intent':\n",
    "                return self.answer_why_intent()\n",
    "            elif found_match and intent == 'about_intellipat':\n",
    "                return self.about_intellipat()\n",
    "            elif found_match and intent == 'about_session':\n",
    "                return self.about_session()\n",
    "        if not found_match:\n",
    "            return self.no_match_intent()\n",
    "    \n",
    "    def describe_planet_intent(self):\n",
    "        responses = (\"My planet is a utopia of diverse organisms and species.\\n\",\n",
    "                     \"I am from Opidipus, the capital of the Wayward Galaxies.\\n\")\n",
    "        return random.choice(responses)\n",
    "    \n",
    "    def answer_why_intent(self):\n",
    "        responses = (\"I come in peace.\\n\",\"I am here to collect data on your planet and its inhabitants\\n\",\n",
    "                     \"I heard the coffee is good.\\n\")\n",
    "        return random.choice(responses)\n",
    "    \n",
    "    def about_intellipat(self):\n",
    "        responses = (\"It is world's largest professional educational company.\\n\",\"It will make you learn concepts in the way never before.\\n\",\n",
    "                     \"It is where your career and your skills grow.\\n\")\n",
    "        return random.choice(responses)\n",
    "\n",
    "    def about_session(self):\n",
    "        responses = (\"Session is on 1 July 2025.\\n\",\"Session was cool\\n\")\n",
    "        return random.choice(responses)\n",
    "    \n",
    "    def no_match_intent(self):\n",
    "        responses = (\"Please tell me more\\n\",\"Tell me more\\n\",\"Why do you say that\\n\",\"I see. Can you elaborate\\n\",\n",
    "                     \"Interesting, can you let me more\\n\",\"I see how do you think\\n\",\"Why?\\n\",\n",
    "                     \"How do you think I feel when you say that?\\n\")\n",
    "        return random.choice(responses)\n",
    "\n",
    "AlienBot = RuleBot()\n",
    "AlienBot.greet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4826593-0cb5-4458-86bf-e0a6b6705d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RETRIEVAL BASED CHATBOT\n",
    "#Steps:-\n",
    "##1. Reading text corpus\n",
    "##2. Preprocessing(Stop words removal, lower case,...)\n",
    "##3. Tokensiation, Stemming & Lemmatization\n",
    "##4. Bag of Words\n",
    "##5. One hot encoding\n",
    "#data.txt is copy and pasted from 'chatbot wikipedia'\n",
    "#if you want more about other data then do the same with other specific data\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3428122b-1a95-40b2-94f4-beaf3cfade43",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data.txt','r',errors = 'ignore')\n",
    "raw_doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4053c06e-9e34-40b6-9846-ffb304054ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dense\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dense\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Dense\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_doc = raw_doc.lower()\n",
    "nltk.download('punkt') #using punkt(twitter) tokeniser\n",
    "nltk.download('wordnet') \n",
    "nltk.download('omw-1.4') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e3ad309-9947-4605-ade8-8fc425880454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Dense\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1c4ae10-e35c-4889-892b-5b63a5a4193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = nltk.sent_tokenize(raw_doc)\n",
    "word_tokens = nltk.word_tokenize(raw_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b524950-6a2e-48b0-973d-7c9f728c5434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nwikipediathe free encyclopedia\\n\\n\\nhide\\nwle austria logo (no text).svg\\nphoto contest wiki loves earth:\\nan international photography competition where you can showcase scotland's unique natural environment and potentially win a prize.\",\n",
       " 'chatbot\\n\\narticle\\ntalk\\n\\ntools\\nfrom wikipedia, the free encyclopedia\\nfor the bot-creation software, see chatbot.',\n",
       " 'for bots on internet relay chat, see irc bot.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be99a217-7d08-484b-8c33-f3dc564948e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wikipediathe', 'free', 'encyclopedia']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "519e2756-b37a-416b-8b9c-6893859af4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation and stop words using Lemmatisation\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de0fc97a-c08d-48c9-87d0-3f451f3217ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define greeting functions\n",
    "\n",
    "greet_inputs = ('hello','hi','whatsup','how are you?')\n",
    "greet_responses = ('hi','hey','hey there','there there')\n",
    "def greet(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in greet_inputs:\n",
    "            return random.choice(greet_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69e68bf9-e18e-4036-ab7d-ada4c8d45c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Response Generation by the bot\n",
    "#cosine similarity-way of measuring similarity of 2 vectors (sentences->vectors)\n",
    "#tfidf(term freq inverse doc freq) Vectoriser-converts text/words into numbers by freq of their appearance(ML models learn from these numbers)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b15e65d3-df29-4593-801d-ae1fd2d1de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo1_response = ''\n",
    "    TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words = 'english')\n",
    "    tfidf = TfidfVec.fit_transform(sentence_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    #returns first most similar element, and within it the second last argument to it\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf == 0):\n",
    "        robo1_response = robo1_response + \"I am sorry. Unable to understand you\"\n",
    "        return robo1_response\n",
    "    else:\n",
    "        robo1_response = robo1_response+sentence_tokens[idx]\n",
    "        return robo1_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d7530db-9065-4902-87e9-6cb293c49360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I am the Retrieval Learning Bot. Start typing your text after greeting to talk to me. For ending convo type bye\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: hey\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Can you tell me about turing test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dense\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Dense\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "\n",
      "history\n",
      "turing test\n",
      "in 1950, alan turing's article \"computing machinery and intelligence\" proposed what is now called the turing test as a criterion of intelligence.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what is universe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I am sorry. Unable to understand you\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Can you tell me about turing test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: [5]\n",
      "\n",
      "history\n",
      "turing test\n",
      "in 1950, alan turing's article \"computing machinery and intelligence\" proposed what is now called the turing test as a criterion of intelligence.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Please say me about turing test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: [5]\n",
      "\n",
      "history\n",
      "turing test\n",
      "in 1950, alan turing's article \"computing machinery and intelligence\" proposed what is now called the turing test as a criterion of intelligence.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Goodbye\n"
     ]
    }
   ],
   "source": [
    "#Defining chat flow\n",
    "flag = True\n",
    "print('Hello I am the Retrieval Learning Bot. Start typing your text after greeting to talk to me. For ending convo type bye')\n",
    "while(flag == True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    if(user_response != 'bye'):\n",
    "        if(user_response =='thank you' or user_response =='thanks'): \n",
    "            flag = False\n",
    "            print('Bot you are welcome')\n",
    "        else:\n",
    "            if(greet(user_response)!=None):\n",
    "                print('Bot: ' + greet(user_response))\n",
    "            else:\n",
    "                sentence_tokens.append(user_response)\n",
    "                word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
    "                final_words = list(set(word_tokens))\n",
    "                print('Bot: ', end = '')\n",
    "                #response will be generated by the logic using cosine similarity and returned\n",
    "                print(response(user_response))\n",
    "                #removing user response from the sentence token(to keep My Corpus clean)\n",
    "                sentence_tokens.remove(user_response)\n",
    "    else:\n",
    "            flag = False\n",
    "            print('Bot: Goodbye')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
